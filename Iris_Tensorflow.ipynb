{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet: Tenserflow\n",
    "\n",
    "## Gerard Naughton G00209309\n",
    "\n",
    "### Objective: \n",
    "These problems relate to the Python package Tensorflow. We will again use the famous iris data set. Save your work as a single Jupyter notebook file in a GitHub repository. Include any required data files, a README, and a gitignore file in the repository.\n",
    "Following our problem sheet we use tensorflow keras and numpy to train a model with the iris data. To recognise different types of iris flower when given sepal width and height and petal width and height. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import neccessary imports to process iris date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as kr\n",
    "import csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using CSV import iris data from iris.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import iris data using csv reader excluding the first row of column headers\n",
    "iris = list(csv.reader(open('data/iris.csv')))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we begin Splitting our data into Testing and Training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a array of values representing sepal length and width and petal length and width of various iris species. These will be our input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes through all the list and takes 4 values(0 to 3) in each array and converts them as np.float and places them in the inputs array\n",
    "inputs= np.array(iris)[:,:4].astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a list of values representing the species of iris flower. These will be our output values ie. setosa versicolor and virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes through the array and takes last values(strings) and places them in the outputs array\n",
    "outputs=np.array(iris)[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert outputs strings to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output strings to unique ints using np.unique.\n",
    "outputs_vals, outputs_ints = np.unique(outputs, return_inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using keras utils we encode our catagory integers as binary catagorial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the category integers as binary categorical vairables.\n",
    "outputs_cats = kr.utils.to_categorical(outputs_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Split up the iris data randomly into training set data and test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input and output data sets into training and test subsets.\n",
    "# create a random list if integers between 0 and 149\n",
    "inds = np.random.permutation(len(inputs))\n",
    "# Split the array into seperate arrays train_inds and test_inds\n",
    "train_inds, test_inds = np.array_split(inds, 2)\n",
    "# Using these inds create a training set of data and test set of data\n",
    "inputs_train, outputs_train = inputs[train_inds], outputs_cats[train_inds]\n",
    "inputs_test,  outputs_test  = inputs[test_inds],  outputs_cats[test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we start creating our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create a neural network\n",
    "First we create our sequential model. A sequential model is a linear stack of layers. We can then add layers to our model using the add() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network.\n",
    "model = kr.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Now we start adding layers. \n",
    "We use the dense layer for our initial layer because we are using 2D arrays. The first value represents the number of nodes in the hidden layer(16) and the input_shape represents our number of input values which is 4. Having researched hidden layers and trying to find the correct way to determine how many nodes should be  in your hidden layer. Factors that are taken into consideration are number of inputs and outputs, number of training cases, complexity of the function and the type of hidden activation function. \n",
    "From reading http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-10.html it tells me there is no concrete rule in determining the best number of hidden units these factors do help but training several networks and estimating results seems the best way(trial and error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an initial layer with 4 input nodes, and a hidden layer with 16 nodes.\n",
    "model.add(kr.layers.Dense(16, input_shape=(4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Adding activation function\n",
    "Having talked about the hidden layer in part 9. I used a array of different activation functions and number of nodes to determinemy best results. Testing a range of hidden layer nodes from 8, 12, 16, 20 with 3 activation function sigmoid tanh and relu and eventually determining 16 nodes and activation function tanh yielded the best results with average of 97%. The tanh function offers stronger gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tanh activation function to that layer.\n",
    "model.add(kr.layers.Activation(\"tanh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Layer\n",
    "Here we create another Dence layer which will have 3 outputs representing either setosa versicolor and virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another layer, connected to the layer with 16 nodes, containing three output nodes.\n",
    "model.add(kr.layers.Dense(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Adding activation function\n",
    "We apply the softmax activation function to the final layer as it can be used to represent catagorical data. Outputing results ranging from 0 upto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the softmax activation function there.\n",
    "model.add(kr.layers.Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Configure model for training\n",
    "Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments: Optimizer, loss and metrics.\n",
    "\n",
    "The Adam optimizer is a optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. Adapted from https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "Categorical_crossentrophy: catagorises our loss with our catagories( setosa, versicolor and virginica.\n",
    "Metrics is a function used to judge the performance of your model and we are using the accyracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model for training.\n",
    "# Uses the adam optimizer and categorical cross entropy as the loss function.\n",
    "# Add in some extra metrics - accuracy being the only one.\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we Train our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit the model for Training\n",
    "For training the model we use the fit function and pass these arguments into its constructor.\n",
    "X representing our inputs_train\n",
    "Y representing our outputs_train\n",
    "epochs which is the number of iterations it will do over the data given\n",
    "batch_size will give us samples per gradient update.\n",
    "verbose = 1 provides us with a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - 0s - loss: 1.4643 - acc: 0.3067     \n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 0s - loss: 1.1106 - acc: 0.4000     \n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 0s - loss: 1.0136 - acc: 0.3867        \n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s - loss: 0.9436 - acc: 0.5200     \n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s - loss: 0.8701 - acc: 0.6933        \n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 0s - loss: 0.8042 - acc: 0.7067         \n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 0s - loss: 0.7423 - acc: 0.7467        \n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 0s - loss: 0.6971 - acc: 0.7600     \n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 0s - loss: 0.6539 - acc: 0.7867     \n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 0s - loss: 0.6135 - acc: 0.8800     \n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 0s - loss: 0.5876 - acc: 0.7733     \n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s - loss: 0.5578 - acc: 0.8400     \n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s - loss: 0.5403 - acc: 0.8133     \n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 0s - loss: 0.5121 - acc: 0.8267     \n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 0s - loss: 0.4948 - acc: 0.9067     \n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s - loss: 0.4797 - acc: 0.8133     \n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s - loss: 0.4723 - acc: 0.8667     \n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s - loss: 0.4492 - acc: 0.8933     \n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 0s - loss: 0.4493 - acc: 0.8800     \n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 0s - loss: 0.4300 - acc: 0.9067     \n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 0s - loss: 0.4094 - acc: 0.9600     \n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s - loss: 0.3928 - acc: 0.9467     \n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s - loss: 0.3822 - acc: 0.9333     \n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s - loss: 0.3809 - acc: 0.8800     \n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 0s - loss: 0.3567 - acc: 0.9600     \n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 0s - loss: 0.3472 - acc: 0.9333     \n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 0s - loss: 0.3445 - acc: 0.9333        \n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s - loss: 0.3198 - acc: 0.9600     \n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 0s - loss: 0.3182 - acc: 0.9600     \n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 0s - loss: 0.3049 - acc: 0.9333     \n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 0s - loss: 0.2966 - acc: 0.9467     \n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 0s - loss: 0.2816 - acc: 0.9333        \n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 0s - loss: 0.2843 - acc: 0.9467     \n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 0s - loss: 0.2739 - acc: 0.9467        \n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s - loss: 0.2548 - acc: 0.9600     \n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s - loss: 0.2492 - acc: 0.9600     \n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s - loss: 0.2402 - acc: 0.9733     \n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s - loss: 0.2359 - acc: 0.9600     \n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s - loss: 0.2271 - acc: 0.9600     \n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 0s - loss: 0.2232 - acc: 0.9600     \n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 0s - loss: 0.2114 - acc: 0.9733     \n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 0s - loss: 0.2184 - acc: 0.9600     \n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s - loss: 0.1972 - acc: 0.9333     \n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s - loss: 0.1876 - acc: 0.9600     \n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s - loss: 0.1960 - acc: 0.9600     \n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s - loss: 0.1892 - acc: 0.9600     \n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 0s - loss: 0.1803 - acc: 0.9467     \n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s - loss: 0.1841 - acc: 0.9600     \n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s - loss: 0.1754 - acc: 0.9600     \n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s - loss: 0.1753 - acc: 0.9733     \n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s - loss: 0.1626 - acc: 0.9733     \n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s - loss: 0.1595 - acc: 0.9600     \n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s - loss: 0.1645 - acc: 0.9733     \n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s - loss: 0.1480 - acc: 0.9733     \n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s - loss: 0.1540 - acc: 0.9600     \n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s - loss: 0.1446 - acc: 0.9600     \n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 0s - loss: 0.1449 - acc: 0.9600     \n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 0s - loss: 0.1397 - acc: 0.9733     \n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 0s - loss: 0.1331 - acc: 0.9600     \n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s - loss: 0.1301 - acc: 0.9733     \n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s - loss: 0.1265 - acc: 0.9733     \n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s - loss: 0.1377 - acc: 0.9733     \n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s - loss: 0.1245 - acc: 0.9733     \n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s - loss: 0.1196 - acc: 0.9733     \n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 0s - loss: 0.1176 - acc: 0.9733     \n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 0s - loss: 0.1179 - acc: 0.9733     \n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s - loss: 0.1097 - acc: 0.9733     \n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 0s - loss: 0.1174 - acc: 0.9733     \n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 0s - loss: 0.1143 - acc: 0.9733     \n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s - loss: 0.1121 - acc: 0.9600        \n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 0s - loss: 0.1097 - acc: 0.9733     \n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 0s - loss: 0.1046 - acc: 0.9733     \n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s - loss: 0.1068 - acc: 0.9733     \n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s - loss: 0.1097 - acc: 0.9600        \n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s - loss: 0.1077 - acc: 0.9733        \n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s - loss: 0.1019 - acc: 0.9733        \n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s - loss: 0.1034 - acc: 0.9733        \n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 0s - loss: 0.0947 - acc: 0.9733     \n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 0s - loss: 0.0949 - acc: 0.9600     \n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 0s - loss: 0.0999 - acc: 0.9733     \n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 0s - loss: 0.0899 - acc: 0.9733     \n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 0s - loss: 0.0953 - acc: 0.9600     \n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 0s - loss: 0.0932 - acc: 0.9600     \n",
      "Epoch 84/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0956 - acc: 0.983 - 0s - loss: 0.0893 - acc: 0.9867     \n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s - loss: 0.0861 - acc: 0.9733     \n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 0s - loss: 0.0857 - acc: 0.9733     \n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 0s - loss: 0.0991 - acc: 0.9600     \n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 0s - loss: 0.0937 - acc: 0.9733     \n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 0s - loss: 0.0863 - acc: 0.9733     \n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s - loss: 0.0868 - acc: 0.9733     \n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s - loss: 0.0860 - acc: 0.9733     \n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 0s - loss: 0.0811 - acc: 0.9867     \n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 0s - loss: 0.0864 - acc: 0.9867     \n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s - loss: 0.0869 - acc: 0.9733     \n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s - loss: 0.0848 - acc: 0.9733     \n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s - loss: 0.0781 - acc: 0.9600     \n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s - loss: 0.0761 - acc: 0.9733     \n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s - loss: 0.0775 - acc: 0.9733     \n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s - loss: 0.0849 - acc: 0.9467     \n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s - loss: 0.0883 - acc: 0.9600     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2498c6f5b70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using our training data.\n",
    "model.fit(inputs_train, outputs_train, epochs=100, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we Test our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Evaluate the model against our test data.\n",
    "Calculate our loss and accuracy of our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/75 [===========>..................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data set.\n",
    "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Prints out our loss and accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss: 0.0833\tAccuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "# Output the accuracy of the model.\n",
    "print(\"\\n\\nLoss: %6.4f\\tAccuracy: %6.4f\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Predict the class of a single flower\n",
    "Tests our model against a single test instance and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: [1 0 0]\tEstimated: [1 0 0]\n",
      "That means it's a setosa\n"
     ]
    }
   ],
   "source": [
    "# Predict the class of a single flower.\n",
    "prediction = np.around(model.predict(np.expand_dims(inputs_test[0], axis=0))).astype(np.int)[0]\n",
    "print(\"Actual: %s\\tEstimated: %s\" % (outputs_test[0].astype(np.int), prediction))\n",
    "print(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Save Model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file for later use.\n",
    "model.save(\"iris_nn.h5\")\n",
    "# Load the model again with: model = load_model(\"iris_nn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
